import os
from typing import List
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
import google.generativeai as genai

# í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ import
from .prompts import (
    get_validation_prompt,
    get_search_query_prompt,
    get_web_search_prompt,
    get_reflection_prompt,
    get_answer_prompt
)

# State ë° ì„¤ì • import
from .state import (
    Product,
    ProductRecommendationState,
    ProductRecommendationConfig,
    get_latest_user_message
)

# ìŠ¤í‚¤ë§ˆ import
from .tools_and_schemas import (
    ValidationResult,
    SearchQueryResult,
    ReflectionResult
)

# Environment setup
from dotenv import load_dotenv

load_dotenv()

if os.getenv("GEMINI_API_KEY") is None:
    raise ValueError("GEMINI_API_KEY is not set")

# Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# ========== ë…¸ë“œ êµ¬í˜„ ==========

# 1. ê²€ì¦ ë…¸ë“œ
def validate_request(state: ProductRecommendationState, config: RunnableConfig) -> dict:
    """ì‚¬ìš©ì ìš”ì²­ì˜ êµ¬ì²´ì„±ì„ ê²€ì¦í•˜ê³  í•„ìš”ì‹œ êµ¬ì²´í™” ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    configurable = ProductRecommendationConfig.from_runnable_config(config)
    
    # LLM ì´ˆê¸°í™”
    llm = ChatGoogleGenerativeAI(
        model=configurable.validation_model,
        temperature=0.1,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY")
    )
    
    # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ ìŠ¤í‚¤ë§ˆ ì ìš©
    structured_llm = llm.with_structured_output(ValidationResult)
    
    # ì‚¬ìš©ì ìš”ì²­ ì¶”ì¶œ
    user_message = get_latest_user_message(state["messages"])
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    validation_prompt = get_validation_prompt(user_message)
    
    # LLM í˜¸ì¶œ
    result = structured_llm.invoke(validation_prompt)
    
    return {
        "is_request_specific": result.is_specific,
        "response_to_user": result.clarification_question if not result.is_specific else "",
        "user_intent": result.extracted_requirements.get("intent", "")
    }


# 2. ê²€ìƒ‰ì–´ ìƒì„± ë…¸ë“œ
def generate_search_queries(state: ProductRecommendationState, config: RunnableConfig) -> dict:
    """êµ¬ì²´í™”ëœ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì–´ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    configurable = ProductRecommendationConfig.from_runnable_config(config)
    
    llm = ChatGoogleGenerativeAI(
        model=configurable.search_model,
        temperature=0.7,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY")
    )
    
    structured_llm = llm.with_structured_output(SearchQueryResult)
    
    user_intent = state.get("user_intent", "")
    user_message = get_latest_user_message(state["messages"])
    
    search_prompt = get_search_query_prompt(user_message, user_intent, configurable.max_search_queries)
    result = structured_llm.invoke(search_prompt)
    
    return {"search_queries": result.queries}


def continue_to_web_search(state: ProductRecommendationState):
    """LangGraphì˜ Send ì´ë²¤íŠ¸ë¥¼ ì‚¬ìš©í•œ ë™ì  ë³‘ë ¬ ê²€ìƒ‰"""
    return [
        Send("web_search", {"search_query": query, "id": int(idx)})
        for idx, query in enumerate(state["search_queries"])
    ]


def web_search(state: dict, config: RunnableConfig) -> dict:
    """Gemini APIì˜ Google Search ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ì œí’ˆ í›„ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    configurable = ProductRecommendationConfig.from_runnable_config(config)
    
    # ë‹¨ì¼ ê²€ìƒ‰ì–´ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)
    query = state["search_query"]
    search_id = state["id"]
    
    try:
        # Geminiì— ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        search_prompt = get_web_search_prompt(query)

        # Gemini API í˜¸ì¶œ (Google Search ë„êµ¬ í¬í•¨)
        model = genai.GenerativeModel(configurable.search_model)
        response = model.generate_content(
            search_prompt,
            tools=[{"google_search": {}}],
            generation_config=genai.types.GenerationConfig(temperature=0.3)
        )
        
        # grounding metadataì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ (quickstart ë°©ì‹)
        sources_gathered = []
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                for chunk in candidate.grounding_metadata.grounding_chunks:
                    if hasattr(chunk, 'web') and chunk.web:
                        sources_gathered.append({
                            "title": chunk.web.title,
                            "url": chunk.web.uri,
                            "search_id": search_id
                        })
        
        # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
        products = extract_products_from_search_result(response.text, sources_gathered)
        
        return {
            "search_queries": [query],
            "candidate_products": products,
            "sources_gathered": sources_gathered
        }
        
    except Exception as e:
        print(f"ê²€ìƒ‰ ì˜¤ë¥˜ ({query}): {e}")
        return {
            "search_queries": [query],
            "candidate_products": [],
            "sources_gathered": []
        }


def extract_products_from_search_result(content: str, sources: List) -> List[Product]:
    """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    products = []
    
    try:
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œí’ˆ ì •ë³´ ì¶”ì¶œ ë¡œì§
        if "ì¶”ì²œ" in content and ("ì œí’ˆ" in content or "ìƒí’ˆ" in content):
            # ê°„ë‹¨í•œ íŒŒì‹± - ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ LLM ê¸°ë°˜ ì¶”ì¶œ í•„ìš”
            lines = content.split('\n')
            current_product = {}
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # ì œí’ˆëª… ì¶”ì¶œ ì‹œë„
                if any(keyword in line for keyword in ['ì¶”ì²œ', 'ë² ìŠ¤íŠ¸', 'ì¸ê¸°', 'ìˆœìœ„']):
                    if current_product and current_product.get('name'):
                        products.append(current_product)
                        current_product = {}
                    
                    current_product = {
                        "name": line[:50],  # ì²« 50ìë§Œ
                        "source_url": sources[0]["url"] if sources else "",
                        "purchase_link": "",
                        "review_summary": content[:200] + "...",
                        "price_range": "ê°€ê²© ì •ë³´ í™•ì¸ í•„ìš”"
                    }
            
            # ë§ˆì§€ë§‰ ì œí’ˆ ì¶”ê°€
            if current_product and current_product.get('name'):
                products.append(current_product)
            
            # ìµœì†Œ 1ê°œëŠ” ë°˜í™˜
            if not products and sources:
                products.append({
                    "name": f"ê²€ìƒ‰ ê²°ê³¼ ì œí’ˆ ({len(sources)}ê°œ ì¶œì²˜)",
                    "source_url": sources[0]["url"],
                    "purchase_link": "",
                    "review_summary": content[:200] + "..." if content else "ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "price_range": "ê°€ê²© ì •ë³´ í™•ì¸ í•„ìš”"
                })
                
    except Exception as e:
        print(f"ì œí’ˆ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    return products[:3]  # ìµœëŒ€ 3ê°œê¹Œì§€


def reflection(state: ProductRecommendationState, config: RunnableConfig) -> dict:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‰ê°€í•˜ê³  ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    
    configurable = ProductRecommendationConfig.from_runnable_config(config)
    
    llm = ChatGoogleGenerativeAI(
        model=configurable.analysis_model,
        temperature=0.1,
        max_retries=2,
        api_key=os.getenv("GEMINI_API_KEY")
    )
    
    # í˜„ì¬ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
    user_message = get_latest_user_message(state["messages"])
    candidate_products = state.get("candidate_products", [])
    search_queries = state.get("search_queries", [])
    
    reflection_prompt = get_reflection_prompt(user_message, candidate_products, search_queries)
    
    structured_llm = llm.with_structured_output(ReflectionResult)
    result = structured_llm.invoke(reflection_prompt)
    
    return {
        "is_sufficient": result.is_sufficient,
        "additional_queries": result.additional_queries if not result.is_sufficient else [],
        "search_loop_count": state.get("search_loop_count", 0) + 1
    }


def should_continue_search(state: ProductRecommendationState) -> str:
    """ì¶”ê°€ ê²€ìƒ‰ í•„ìš”ì„±ì— ë”°ë¥¸ ë¼ìš°íŒ… ê²°ì •"""
    max_loops = state.get("max_search_loops", 2)
    current_loop = state.get("search_loop_count", 0)
    
    if state.get("is_sufficient", False) or current_loop >= max_loops:
        return "format_response"
    else:
        return "generate_search_queries"


def format_response(state: ProductRecommendationState, config: RunnableConfig) -> dict:
    """ìµœì¢… ì¶”ì²œ ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•íƒœë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    
    user_message = get_latest_user_message(state["messages"])
    candidate_products = state.get("candidate_products", [])
    
    if not candidate_products:
        response = "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ëŠ” ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ê²€ìƒ‰í•´ë³´ì‹œê² ì–´ìš”?"
    else:
        response = format_product_recommendations(user_message, candidate_products)
    
    return {
        "response_to_user": response,
        "messages": [AIMessage(content=response)]
    }


def format_product_recommendations(user_request: str, products: List[Product]) -> str:
    """ì œí’ˆ ì¶”ì²œ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    
    response = f"**'{user_request}'** ìš”ì²­ì— ëŒ€í•œ ì¶”ì²œ ì œí’ˆì…ë‹ˆë‹¤! ğŸ¯\n\n"
    
    for i, product in enumerate(products[:5], 1):  # ìµœëŒ€ 5ê°œê¹Œì§€
        response += f"## {i}. {product['name']}\n\n"
        
        if product.get('price_range'):
            response += f"ğŸ’° **ê°€ê²©ëŒ€**: {product['price_range']}\n\n"
        
        if product.get('review_summary'):
            response += f"ğŸ“ **ì œí’ˆ ì •ë³´**:\n{product['review_summary']}\n\n"
        
        if product.get('purchase_link'):
            response += f"ğŸ›’ [êµ¬ë§¤í•˜ëŸ¬ ê°€ê¸°]({product['purchase_link']})\n\n"
        
        if product.get('source_url'):
            response += f"ğŸ“š [ìƒì„¸ ì •ë³´ ë³´ê¸°]({product['source_url']})\n\n"
        
        response += "---\n\n"
    
    response += "ğŸ’¡ **ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!**"
    
    return response


def should_refine_or_search(state: ProductRecommendationState) -> str:
    """ìš”ì²­ì˜ êµ¬ì²´ì„±ì— ë”°ë¥¸ ë¼ìš°íŒ… ê²°ì •"""
    return "search" if state.get("is_request_specific", False) else "refine"

# ========== ê·¸ë˜í”„ êµ¬ì„± ==========

def create_product_recommendation_graph():
    """ì œí’ˆ ì¶”ì²œ ê·¸ë˜í”„ ìƒì„±"""
    
    # ê·¸ë˜í”„ ë¹Œë” ì´ˆê¸°í™”
    builder = StateGraph(ProductRecommendationState, config_schema=ProductRecommendationConfig)
    
    # ë…¸ë“œ ì¶”ê°€
    builder.add_node("validate_request", validate_request)
    builder.add_node("generate_search_queries", generate_search_queries)
    builder.add_node("web_search", web_search)
    builder.add_node("reflection", reflection)
    builder.add_node("format_response", format_response)
    
    # ì—£ì§€ êµ¬ì„±
    builder.add_edge(START, "validate_request")
    builder.add_conditional_edges(
        "validate_request",
        should_refine_or_search,
        {
            "refine": END,  # êµ¬ì²´í™” ì§ˆë¬¸ìœ¼ë¡œ ì¢…ë£Œ
            "search": "generate_search_queries"  # ê²€ìƒ‰ ì§„í–‰
        }
    )
    builder.add_conditional_edges("generate_search_queries", continue_to_web_search, ["web_search"])
    builder.add_edge("web_search", "reflection")
    builder.add_conditional_edges(
        "reflection",
        should_continue_search,
        {
            "generate_search_queries": "generate_search_queries",  # ì¶”ê°€ ê²€ìƒ‰ í•„ìš”
            "format_response": "format_response"  # ê²€ìƒ‰ ì™„ë£Œ
        }
    )
    builder.add_edge("format_response", END)
    
    return builder.compile()

# ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (quickstart ë°©ì‹)
#graph = create_product_recommendation_graph()

# í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ êµ¬ì„±
def create_test_product_recommendation_graph():
    """ì œí’ˆ ì¶”ì²œ ê·¸ë˜í”„ ìƒì„±"""
    
    # ê·¸ë˜í”„ ë¹Œë” ì´ˆê¸°í™”
    builder = StateGraph(ProductRecommendationState, config_schema=ProductRecommendationConfig)
    
    # ë…¸ë“œ ì¶”ê°€
    builder.add_node("validate_request", validate_request)
    builder.add_node("generate_search_queries", generate_search_queries)

    # ì—£ì§€ êµ¬ì„±
    builder.add_edge(START, "validate_request")
    builder.add_conditional_edges(
        "validate_request",
        should_refine_or_search,
        {
            "refine": END,  # êµ¬ì²´í™” ì§ˆë¬¸ìœ¼ë¡œ ì¢…ë£Œ
            "search": "generate_search_queries"  # ê²€ìƒ‰ ì§„í–‰
        }
    )
    
    builder.add_edge("generate_search_queries", END)
    
    return builder.compile()

graph = create_test_product_recommendation_graph()